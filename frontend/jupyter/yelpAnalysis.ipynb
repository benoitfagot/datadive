{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = pd.read_json(\"./business.json\", lines=True)\n",
    "\n",
    "#Take 100K records from the dataset\n",
    "df_business = df_business.iloc[0:100000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = pd.read_json(\"./review75k.json\", lines=True)\n",
    "#Take 50K records from the dataset\n",
    "df_review = df_review.iloc[0:50000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_business.shape)\n",
    "df_business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q1sbwvVQXV2734tPgoKj4Q</td>\n",
       "      <td>hG7b0MtEbXx5QzbzE6C_VA</td>\n",
       "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>GJXCdrto3ASJOqKeVWPi6Q</td>\n",
       "      <td>yXQM5uF2jS6es16SJzNHfg</td>\n",
       "      <td>NZnhc2sEQy3RmzKTZnqtwQ</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "      <td>2017-01-14 21:30:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2TzJjDVDEuAW6MR5Vuc1ug</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>yi0R0Ugj_xUx_Nek0-_Qig</td>\n",
       "      <td>dacAIZ6fTM6mqwW5uxkskg</td>\n",
       "      <td>ikCg8xy5JIg_NGPx-MSIDA</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "      <td>2018-01-09 20:56:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11a8sVPMUFtaC7_ABRkmtw</td>\n",
       "      <td>ssoyf2_x0EQMed6fgHeMyQ</td>\n",
       "      <td>b1b1eb3uo-w561D0ZfCEiQ</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "      <td>2018-01-30 23:07:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  Q1sbwvVQXV2734tPgoKj4Q  hG7b0MtEbXx5QzbzE6C_VA  ujmEBvifdJM6h6RLv4wQIg   \n",
       "1  GJXCdrto3ASJOqKeVWPi6Q  yXQM5uF2jS6es16SJzNHfg  NZnhc2sEQy3RmzKTZnqtwQ   \n",
       "2  2TzJjDVDEuAW6MR5Vuc1ug  n6-Gk65cPZL6Uz8qRm3NYw  WTqjgwHlXbSFevF32_DJVw   \n",
       "3  yi0R0Ugj_xUx_Nek0-_Qig  dacAIZ6fTM6mqwW5uxkskg  ikCg8xy5JIg_NGPx-MSIDA   \n",
       "4  11a8sVPMUFtaC7_ABRkmtw  ssoyf2_x0EQMed6fgHeMyQ  b1b1eb3uo-w561D0ZfCEiQ   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      1       6      1     0   \n",
       "1      5       0      0     0   \n",
       "2      5       3      0     0   \n",
       "3      5       0      0     0   \n",
       "4      1       7      0     0   \n",
       "\n",
       "                                                text                date  \n",
       "0  Total bill for this horrible service? Over $8G... 2013-05-07 04:34:36  \n",
       "1  I *adore* Travis at the Hard Rock's new Kelly ... 2017-01-14 21:30:33  \n",
       "2  I have to say that this office really has it t... 2016-11-09 20:09:03  \n",
       "3  Went in for a lunch. Steak sandwich was delici... 2018-01-09 20:56:38  \n",
       "4  Today was my second out of three sessions I ha... 2018-01-30 23:07:38  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_review.shape)\n",
    "df_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value this shit bugs me so much OMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id        0\n",
       "name               0\n",
       "address            0\n",
       "city               0\n",
       "state              0\n",
       "postal_code        0\n",
       "latitude           0\n",
       "longitude          0\n",
       "stars              0\n",
       "review_count       0\n",
       "is_open            0\n",
       "attributes      1121\n",
       "categories         0\n",
       "hours           7304\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_business.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duplicate business_id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_business.loc[:,'business_id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove quotation marks which can cause parsing problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business['name'] = df_business['name'].str.replace('\"', '')\n",
    "df_business['address'] = df_business['address'].str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the sake of simplicity, I prefer to analyse datas related to restaurants first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurant = df.loc[df['categories'].str.contains('Restaurants') == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Redefinied the category of each restaurant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurant['category'] = pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurant.loc[df_restaurant.categories.str.contains('American'), 'category'] = 'American'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('Mexican'), 'category'] = 'Mexican'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('Italian'), 'category'] = 'Italian'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('Japanese'), 'category'] = 'Japanese'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('Chinese'), 'category'] = 'Chinese'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('Vietnamese'), 'category'] = 'Vietnamese'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('Thai'), 'category'] = 'Thai'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('Indian'), 'category'] = 'Indian'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('French'), 'category'] = 'French'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('African'), 'category'] = 'African'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('Spanish'), 'category'] = 'Spanish'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('Greek'), 'category'] = 'Greek'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('Mediterranean'), 'category'] = 'Mediterranean'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('Middle_eastern'), 'category'] = 'Middle_eastern'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('Korean'), 'category'] = 'Korean'\n",
    "df_restaurant.loc[df_restaurant.categories.str.contains('Hawaiian'), 'category'] = 'Hawaiian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id         0\n",
       "name                0\n",
       "address             0\n",
       "city                0\n",
       "state               0\n",
       "postal_code         0\n",
       "latitude            0\n",
       "longitude           0\n",
       "stars               0\n",
       "review_count        0\n",
       "is_open             0\n",
       "attributes       1121\n",
       "categories          0\n",
       "hours            7304\n",
       "category        13435\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_restaurant.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop null values in category\n",
    "df_restaurant = df_restaurant.dropna(axis=0, subset=['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17332, 15)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_restaurant.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
       "      <td>Emerald Chinese Restaurant</td>\n",
       "      <td>30 Eglinton Avenue W</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>ON</td>\n",
       "      <td>L5R 3E7</td>\n",
       "      <td>43.605499</td>\n",
       "      <td>-79.652289</td>\n",
       "      <td>2.5</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>{'RestaurantsReservations': 'True', 'GoodForMe...</td>\n",
       "      <td>Specialty Food, Restaurants, Dim Sum, Imported...</td>\n",
       "      <td>{'Monday': '9:0-0:0', 'Tuesday': '9:0-0:0', 'W...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
       "      <td>Musashi Japanese Restaurant</td>\n",
       "      <td>10110 Johnston Rd, Ste 15</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>28210</td>\n",
       "      <td>35.092564</td>\n",
       "      <td>-80.859132</td>\n",
       "      <td>4.0</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>{'GoodForKids': 'True', 'NoiseLevel': 'u'avera...</td>\n",
       "      <td>Sushi Bars, Restaurants, Japanese</td>\n",
       "      <td>{'Monday': '17:30-21:30', 'Wednesday': '17:30-...</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1Dfx3zM-rW4n-31KeC8sJg</td>\n",
       "      <td>Taco Bell</td>\n",
       "      <td>2450 E Indian School Rd</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85016</td>\n",
       "      <td>33.495194</td>\n",
       "      <td>-112.028588</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>{'RestaurantsTakeOut': 'True', 'BusinessParkin...</td>\n",
       "      <td>Restaurants, Breakfast &amp; Brunch, Mexican, Taco...</td>\n",
       "      <td>{'Monday': '7:0-0:0', 'Tuesday': '7:0-0:0', 'W...</td>\n",
       "      <td>Mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>fweCYi8FmbJXHCqLnwuk8w</td>\n",
       "      <td>Marco's Pizza</td>\n",
       "      <td>5981 Andrews Rd</td>\n",
       "      <td>Mentor-on-the-Lake</td>\n",
       "      <td>OH</td>\n",
       "      <td>44060</td>\n",
       "      <td>41.708520</td>\n",
       "      <td>-81.359556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>{'RestaurantsPriceRange2': '2', 'BusinessAccep...</td>\n",
       "      <td>Italian, Restaurants, Pizza, Chicken Wings</td>\n",
       "      <td>{'Monday': '10:0-0:0', 'Tuesday': '10:0-0:0', ...</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>PZ-LZzSlhSe9utkQYU8pFg</td>\n",
       "      <td>Carluccio's Tivoli Gardens</td>\n",
       "      <td>1775 E Tropicana Ave, Ste 29</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>89119</td>\n",
       "      <td>36.100016</td>\n",
       "      <td>-115.128529</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>{'OutdoorSeating': 'False', 'BusinessAcceptsCr...</td>\n",
       "      <td>Restaurants, Italian</td>\n",
       "      <td>None</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               business_id                         name  \\\n",
       "1   QXAEGFB4oINsVuTFxEYKFQ   Emerald Chinese Restaurant   \n",
       "2   gnKjwL_1w79qoiV3IC_xQQ  Musashi Japanese Restaurant   \n",
       "11  1Dfx3zM-rW4n-31KeC8sJg                    Taco Bell   \n",
       "13  fweCYi8FmbJXHCqLnwuk8w                Marco's Pizza   \n",
       "17  PZ-LZzSlhSe9utkQYU8pFg   Carluccio's Tivoli Gardens   \n",
       "\n",
       "                         address                city state postal_code  \\\n",
       "1           30 Eglinton Avenue W         Mississauga    ON     L5R 3E7   \n",
       "2      10110 Johnston Rd, Ste 15           Charlotte    NC       28210   \n",
       "11       2450 E Indian School Rd             Phoenix    AZ       85016   \n",
       "13               5981 Andrews Rd  Mentor-on-the-Lake    OH       44060   \n",
       "17  1775 E Tropicana Ave, Ste 29           Las Vegas    NV       89119   \n",
       "\n",
       "     latitude   longitude  stars  review_count  is_open  \\\n",
       "1   43.605499  -79.652289    2.5           128        1   \n",
       "2   35.092564  -80.859132    4.0           170        1   \n",
       "11  33.495194 -112.028588    3.0            18        1   \n",
       "13  41.708520  -81.359556    4.0            16        1   \n",
       "17  36.100016 -115.128529    4.0            40        0   \n",
       "\n",
       "                                           attributes  \\\n",
       "1   {'RestaurantsReservations': 'True', 'GoodForMe...   \n",
       "2   {'GoodForKids': 'True', 'NoiseLevel': 'u'avera...   \n",
       "11  {'RestaurantsTakeOut': 'True', 'BusinessParkin...   \n",
       "13  {'RestaurantsPriceRange2': '2', 'BusinessAccep...   \n",
       "17  {'OutdoorSeating': 'False', 'BusinessAcceptsCr...   \n",
       "\n",
       "                                           categories  \\\n",
       "1   Specialty Food, Restaurants, Dim Sum, Imported...   \n",
       "2                   Sushi Bars, Restaurants, Japanese   \n",
       "11  Restaurants, Breakfast & Brunch, Mexican, Taco...   \n",
       "13         Italian, Restaurants, Pizza, Chicken Wings   \n",
       "17                               Restaurants, Italian   \n",
       "\n",
       "                                                hours  category  \n",
       "1   {'Monday': '9:0-0:0', 'Tuesday': '9:0-0:0', 'W...   Chinese  \n",
       "2   {'Monday': '17:30-21:30', 'Wednesday': '17:30-...  Japanese  \n",
       "11  {'Monday': '7:0-0:0', 'Tuesday': '7:0-0:0', 'W...   Mexican  \n",
       "13  {'Monday': '10:0-0:0', 'Tuesday': '10:0-0:0', ...   Italian  \n",
       "17                                               None   Italian  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_restaurant.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPLACE_NEW_LINE = re.compile(\"\\s*\\n+\\s*\")\n",
    "#REPLACE_NO_SPACE = re.compile('[.;:!\\'?,\\\"()\\[\\]]')\n",
    "#stopword = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lower all text\n",
    "def toLower(text):\n",
    "    return text.lower()\n",
    "\n",
    "#Replace all new line, basically text's shown in the same paragraph\n",
    "def noNewLine(text):\n",
    "    REPLACE_NEW_LINE = re.compile('\\s*\\n+\\s*')\n",
    "    text = REPLACE_NEW_LINE.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "#Weird character as the title said\n",
    "def weirdCharacter(text):\n",
    "    REPLACE_NO_SPACE = re.compile('[!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~]')\n",
    "    text = REPLACE_NO_SPACE.sub('', text)\n",
    "    return text\n",
    "\n",
    "#Word tokenizer\n",
    "def tokenizing(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "#Remove stopword\n",
    "def removeStopWords(tokens):\n",
    "    swords = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in swords]\n",
    "    return tokens\n",
    "    \n",
    "#Remove empty token\n",
    "#This can happen when parse double whitespace or some unexpected comment\n",
    "def removeEmptyToken(tokens):\n",
    "    tokens = [token for token in tokens if len(token) > 0]\n",
    "    return tokens\n",
    "\n",
    "#Join token together into text paragraph\n",
    "def joinToken(tokens):\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine all text preprocessing methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_text(text):\n",
    "    text = noNewLine(text)\n",
    "    text = toLower(text)\n",
    "    text = weirdCharacter(text)\n",
    "    tokens = tokenizing(text)\n",
    "    tokens = removeStopWords(tokens)\n",
    "    tokens = removeEmptyToken(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def testReformat(text):\n",
    "#    text = noNewLine(text)\n",
    "#    text = toLower(text)\n",
    "#    text = weirdCharacter(text)\n",
    "#    tokens = tokenizing(text)\n",
    "#    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CC coordinating conjunction\n",
    "- CD cardinal digit\n",
    "- DT determiner\n",
    "- EX existential there (like: “there is” … think of it like “there exists”)\n",
    "- FW foreign word\n",
    "- IN preposition/subordinating conjunction\n",
    "- JJ adjective ‘big’\n",
    "- JJR adjective, comparative ‘bigger’\n",
    "- JJS adjective, superlative ‘biggest’\n",
    "- LS list marker 1\n",
    "- MD modal could, will\n",
    "- NN noun, singular ‘desk’\n",
    "- NNS noun plural ‘desks’\n",
    "- NNP proper noun, singular ‘Harrison’\n",
    "- NNPS proper noun, plural ‘Americans’\n",
    "- PDT predeterminer ‘all the kids’\n",
    "- POS possessive ending parent’s\n",
    "- PRP personal pronoun I, he, she\n",
    "- PRP possessive pronoun my, his, hers\n",
    "- RB adverb very, silently,\n",
    "- RBR adverb, comparative better\n",
    "- RBS adverb, superlative best\n",
    "- RP particle give up\n",
    "- TO, to go ‘to’ the store.\n",
    "- UH interjection, errrrrrrrm\n",
    "- VB verb, base form take\n",
    "- VBD verb, past tense took\n",
    "- VBG verb, gerund/present participle taking\n",
    "- VBN verb, past participle taken\n",
    "- VBP verb, sing. present, non-3d take\n",
    "- VBZ verb, 3rd person sing. present takes\n",
    "- WDT wh-determiner which\n",
    "- WP wh-pronoun who, what\n",
    "- WP possessive wh-pronoun whose\n",
    "- WRB wh-abverb where, when**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS (Part Of Speech) tagging** \\\n",
    "Assign a tag to every word to define if it corresponds to a noun, a verb, etc using Wordnet lexical database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(tokens):\n",
    "    #Incase some idiots dont parse a tokens in the parameter\n",
    "    if not isinstance(tokens, (list, tuple)):\n",
    "        tokens = tokenizing(tokens)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        print(tag)\n",
    "        if tag.startswith('NN' ):\n",
    "            pos = wordnet.NOUN \n",
    "        elif tag.startswith('VB'):\n",
    "            pos = wordnet.VERB\n",
    "        elif tag.startswith('RB'):\n",
    "            pos = wordnet.ADV\n",
    "        elif tag.startswith('JJ'):\n",
    "            pos = wordnet.ADJ\n",
    "        else:\n",
    "            pos = wordnet.NOUN\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = 'I missed the moment we spent together with her, there is still flashbacks of her in my life'\n",
    "#lemmatize_sentence(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokens = reformat_text(text)\n",
    "    tokens = lemmatize_sentence(tokens)\n",
    "    text = joinToken(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply text processing for the review column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['review_clean'] = df_review['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label outcome base on the star column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['outcome'] = pd.Series()\n",
    "df_review.loc[df_review['stars'] > 3, 'outcome'] = 2\n",
    "df_review.loc[df_review['stars'] == 3, 'outcome'] = 1\n",
    "df_review.loc[df_review['stars'] < 3, 'outcome'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q1sbwvVQXV2734tPgoKj4Q</td>\n",
       "      <td>hG7b0MtEbXx5QzbzE6C_VA</td>\n",
       "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "      <td>total bill horrible service 8gs crook actually...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>GJXCdrto3ASJOqKeVWPi6Q</td>\n",
       "      <td>yXQM5uF2jS6es16SJzNHfg</td>\n",
       "      <td>NZnhc2sEQy3RmzKTZnqtwQ</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "      <td>2017-01-14 21:30:33</td>\n",
       "      <td>adore travis hard rock 's new kelly cardenas s...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2TzJjDVDEuAW6MR5Vuc1ug</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>say office really together organize friendly d...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>yi0R0Ugj_xUx_Nek0-_Qig</td>\n",
       "      <td>dacAIZ6fTM6mqwW5uxkskg</td>\n",
       "      <td>ikCg8xy5JIg_NGPx-MSIDA</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "      <td>2018-01-09 20:56:38</td>\n",
       "      <td>go lunch steak sandwich delicious caesar salad...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11a8sVPMUFtaC7_ABRkmtw</td>\n",
       "      <td>ssoyf2_x0EQMed6fgHeMyQ</td>\n",
       "      <td>b1b1eb3uo-w561D0ZfCEiQ</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "      <td>2018-01-30 23:07:38</td>\n",
       "      <td>today second three session pay although first ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  Q1sbwvVQXV2734tPgoKj4Q  hG7b0MtEbXx5QzbzE6C_VA  ujmEBvifdJM6h6RLv4wQIg   \n",
       "1  GJXCdrto3ASJOqKeVWPi6Q  yXQM5uF2jS6es16SJzNHfg  NZnhc2sEQy3RmzKTZnqtwQ   \n",
       "2  2TzJjDVDEuAW6MR5Vuc1ug  n6-Gk65cPZL6Uz8qRm3NYw  WTqjgwHlXbSFevF32_DJVw   \n",
       "3  yi0R0Ugj_xUx_Nek0-_Qig  dacAIZ6fTM6mqwW5uxkskg  ikCg8xy5JIg_NGPx-MSIDA   \n",
       "4  11a8sVPMUFtaC7_ABRkmtw  ssoyf2_x0EQMed6fgHeMyQ  b1b1eb3uo-w561D0ZfCEiQ   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      1       6      1     0   \n",
       "1      5       0      0     0   \n",
       "2      5       3      0     0   \n",
       "3      5       0      0     0   \n",
       "4      1       7      0     0   \n",
       "\n",
       "                                                text                date  \\\n",
       "0  Total bill for this horrible service? Over $8G... 2013-05-07 04:34:36   \n",
       "1  I *adore* Travis at the Hard Rock's new Kelly ... 2017-01-14 21:30:33   \n",
       "2  I have to say that this office really has it t... 2016-11-09 20:09:03   \n",
       "3  Went in for a lunch. Steak sandwich was delici... 2018-01-09 20:56:38   \n",
       "4  Today was my second out of three sessions I ha... 2018-01-30 23:07:38   \n",
       "\n",
       "                                        review_clean  outcome  \n",
       "0  total bill horrible service 8gs crook actually...      0.0  \n",
       "1  adore travis hard rock 's new kelly cardenas s...      2.0  \n",
       "2  say office really together organize friendly d...      2.0  \n",
       "3  go lunch steak sandwich delicious caesar salad...      2.0  \n",
       "4  today second three session pay although first ...      0.0  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features and label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df_review['review_clean'])\n",
    "y = np.array(df_review['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 forms of vectorization:\n",
    "- Count Vectorization (Same behavior as one hot encoding)\n",
    "- n-grams\n",
    "- TF-IDF\n",
    "\n",
    "24/03: We test TF-IDF which is the most relevant, keep n-gram for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes** \\\n",
    "Practically speaking TF-IDF requires the Bag of Words transformation, luckily for us the implementation of TF-IDF in Scikit-Learn does all of the steps neccessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(reviewFeature)\n",
    "X = tfidf_vectorizer.transform(reviewFeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tunning eventually (For science)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_features: Use n numbers of most frequently occuring words to create a bag of words\n",
    "#max_df: words that occur in a maximum n(num/%) of the document\n",
    "#min_df: words that occur in at least n(num/%) of the document\n",
    "\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_features=3000, max_df=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phucvu/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/phucvu/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.66528\n",
      "[[  42    0 2779]\n",
      " [   1    0 1404]\n",
      " [   0    0 8274]]\n",
      "Accuracy for C=0.05: 0.7604\n",
      "[[1278    0 1543]\n",
      " [ 113    1 1291]\n",
      " [  48    0 8226]]\n",
      "Accuracy for C=0.25: 0.82264\n",
      "[[2068   35  718]\n",
      " [ 239  102 1064]\n",
      " [ 138   23 8113]]\n",
      "Accuracy for C=0.5: 0.83704\n",
      "[[2218   59  544]\n",
      " [ 273  174  958]\n",
      " [ 153   50 8071]]\n",
      "Accuracy for C=1: 0.84336\n",
      "[[2273   80  468]\n",
      " [ 301  230  874]\n",
      " [ 158   77 8039]]\n"
     ]
    }
   ],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    \n",
    "    # I dont know why the fuck Sklearn wants to do this reshape thing but here we are\n",
    "    #y_test = y_test.reshape(1, -1)\n",
    "    #y_pred = y_pred.reshape(1, -1)\n",
    "    \n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, y_pred)))\n",
    "    \n",
    "    print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.79288\n",
      "[[1707    2 1112]\n",
      " [ 184   17 1204]\n",
      " [  84    3 8187]]\n",
      "Accuracy for C=0.05: 0.83576\n",
      "[[2235   43  543]\n",
      " [ 298  117  990]\n",
      " [ 155   24 8095]]\n",
      "Accuracy for C=0.25: 0.84624\n",
      "[[2338   87  396]\n",
      " [ 331  243  831]\n",
      " [ 184   93 7997]]\n",
      "Accuracy for C=0.5: 0.84496\n",
      "[[2335  107  379]\n",
      " [ 345  272  788]\n",
      " [ 198  121 7955]]\n",
      "Accuracy for C=1: 0.8408\n",
      "[[2320  134  367]\n",
      " [ 348  310  747]\n",
      " [ 214  180 7880]]\n"
     ]
    }
   ],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    svm = LinearSVC(C=c)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, y_pred)))\n",
    "    \n",
    "    print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radom Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8408\n",
      "[[2320  134  367]\n",
      " [ 348  310  747]\n",
      " [ 214  180 7880]]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "print (\"Accuracy: %s\" \n",
    "           % (accuracy_score(y_test, y_pred)))\n",
    "    \n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
