{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "pathModulesES = '../sauceforyall/'\n",
    "sys.path.append(pathModulesES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elasticsearch Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yelpquery import YelpQuery\n",
    "from pandasticsearch import Select\n",
    "ye = YelpQuery()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Others**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Index name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_business = \"yelp-business*\"\n",
    "index_review = \"yelp-review*\"\n",
    "index_tip = \"yelp-tip*\"\n",
    "index_user = \"yelp-user*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve all the reviews from the last 3 years, this helps reduce the volume of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstReview = ye.getRangeTerm(index=index_review, term=\"date\", beginDate=\"2016-01-01T00:00:00.000\",endDate=\"2019-12-31T23:59:59.000\", size=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = ye.getResultScrolling(firstReview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Remove Carriage Return and Newline Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_carriage(text):\n",
    "    REPLACE_NEW_LINE = re.compile('\\s*\\n+\\s*')\n",
    "    text = REPLACE_NEW_LINE.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_spaces(text):\n",
    "    REPLACE_SPACE = re.compile('\\s+')\n",
    "    text = REPLACE_SPACE.sub(' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Remove Strange Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_weird_character(text):\n",
    "    REPLACE_NO_SPACE = re.compile('[!\"#$%&\\()/<=>@[\\\\]^_`{|}~]')\n",
    "    #REPLACE_NO_SPACE = re.compile('[!\"#$%&\\()*+-/<=>?@[\\\\]^_`{|}~]')\n",
    "    text = REPLACE_NO_SPACE.sub(' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Remove Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(text):\n",
    "    patterns = [\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'I\\'m', 'I am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would'),\n",
    "    ]\n",
    "    \n",
    "    for (pattern, repl) in patterns:\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Remove Repeated Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/hongphuc95/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/hongphuc95/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hongphuc95/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated(word):\n",
    "    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    repl = r'\\1\\2\\3'\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    \n",
    "    repl_word = repeat_regexp.sub(repl, word)\n",
    "    \n",
    "    if repl_word != word:\n",
    "        return remove_repeated(repl_word)\n",
    "    else:\n",
    "        return repl_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Check Spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO\n",
    "def spelling(word):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_sentence(document):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    \n",
    "    tagged_sentences = []\n",
    "    for sent in sentences:\n",
    "        sent = nltk.word_tokenize(sent)\n",
    "        corrected_words = []\n",
    "        for word in sent:\n",
    "            word = remove_repeated(word)\n",
    "            #word = spelling(word)\n",
    "            word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "            corrected_words.append(word)\n",
    "        \n",
    "        corrected_words = \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in corrected_words]).strip()      \n",
    "        tagged_sentences.append(corrected_words)\n",
    "        \n",
    "    tagged_sentences = \" \".join(tagged_sentences)\n",
    "    \n",
    "    #lemmatized_sentence = []\n",
    "    #for word, tag in pos_tag(tokens):\n",
    "    #    if tag.startswith('NN' ):\n",
    "    #        pos = wordnet.NOUN \n",
    "    #    elif tag.startswith('VB'):\n",
    "    #        pos = wordnet.VERB\n",
    "    #    elif tag.startswith('RB'):\n",
    "    #        pos = wordnet.ADV\n",
    "    #    elif tag.startswith('JJ'):\n",
    "    #        pos = wordnet.ADJ\n",
    "    #    else:\n",
    "    #        pos = wordnet.NOUN\n",
    "    #    lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove carriage\n",
    "df_review[\"text\"] = df_review[\"text\"].apply(lambda x: remove_carriage(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove weird characters\n",
    "df_review[\"text\"] = df_review[\"text\"].apply(lambda x: remove_weird_character(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove double whitespace\n",
    "df_review[\"text\"] = df_review[\"text\"].apply(lambda x: remove_spaces(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove contractions\n",
    "df_review[\"text\"] = df_review[\"text\"].apply(lambda x: remove_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "df_review[\"text\"] = df_review[\"text\"].apply(lambda x: lemmatize_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = df_review.drop(\"_index\", axis = 1)\n",
    "df_review = df_review.drop(\"_type\", axis = 1)\n",
    "df_review = df_review.drop(\"_id\", axis = 1)\n",
    "df_review = df_review.drop(\"@timestamp\", axis = 1)\n",
    "df_review = df_review.drop(\"@version\", axis = 1)\n",
    "df_review = df_review.drop(\"_score\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review.to_json(\"./review_cleaned_2016_2019.json\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
